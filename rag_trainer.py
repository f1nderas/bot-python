import os
import tempfile
from pathlib import Path
from datetime import datetime
from typing import Optional, List
from aiogram import Bot
from aiogram.types import Message, ReplyKeyboardMarkup, KeyboardButton
import pdfplumber
import sqlite3
import logging
import requests
from nltk.tokenize import sent_tokenize
import nltk

# –ó–∞–≥—Ä—É–∂–∞–µ–º —Ä–µ—Å—É—Ä—Å—ã –¥–ª—è NLTK
try:
    nltk.data.find('tokenizers/punkt_tab')
except LookupError:
    nltk.download('punkt_tab', quiet=True)

logger = logging.getLogger(__name__)

class RAGTrainer:
    def __init__(self, db_path: str = 'nutrition_bot.db', llm_url: str = "http://localhost:11434/api/generate"):
        self.db_path = db_path
        self.llm_url = llm_url
        self._init_db()
        logger.info("RAGTrainer –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Å –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö: %s", db_path)

    def _init_db(self):
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute('''CREATE TABLE IF NOT EXISTS training_files (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        filename TEXT NOT NULL,
                        file_type TEXT NOT NULL,
                        processed_at DATETIME NOT NULL,
                        chunks_count INTEGER NOT NULL)''')
        conn.commit()
        conn.close()
        logger.info("–¢–∞–±–ª–∏—Ü–∞ training_files –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")

    async def process_training_file(self, message: Message, bot: Bot, generate_qa=False) -> str:
        logger.info("–ù–∞—á–∞—Ç–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞: %s (ID: %s)", message.document.file_name, message.document.file_id)
        try:
            with tempfile.TemporaryDirectory() as temp_dir:
                file_ext = Path(message.document.file_name).suffix.lower()
                original_file_type = 'pdf' if file_ext == '.pdf' else 'txt'
                file_path = os.path.join(temp_dir, f"training{file_ext}")
                logger.debug("–°–æ–∑–¥–∞–Ω –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª: %s", file_path)

                file_info = await bot.get_file(message.document.file_id)
                downloaded_file = await bot.download_file(file_info.file_path)
                logger.info("–§–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω, —Ä–∞–∑–º–µ—Ä: %s –±–∞–π—Ç", len(downloaded_file.getbuffer()))

                with open(file_path, 'wb') as f:
                    f.write(downloaded_file.read())
                logger.debug("–§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤–æ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏")

                if original_file_type == 'pdf':
                    text_chunks = self._process_large_pdf(file_path)
                else:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        text = f.read()
                    logger.info("–ò–∑–≤–ª–µ—á—ë–Ω —Ç–µ–∫—Å—Ç –∏–∑ TXT: %s —Å–∏–º–≤–æ–ª–æ–≤", len(text))
                    text_chunks = self._split_text_into_chunks(text, chunk_size=2000)

                total_chunks = 0
                total_qa_pairs = 0
                from rag_handler import RAGHandler
                rag = RAGHandler()
                logger.debug("RAGHandler –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —á–∞–Ω–∫–æ–≤")

                for chunk, tags in text_chunks:
                    logger.info("–î–æ–±–∞–≤–ª—è–µ—Ç—Å—è —á–∞–Ω–∫: %s —Å–∏–º–≤–æ–ª–æ–≤, —Ç–µ–≥–∏: %s", len(chunk), tags or "–Ω–µ—Ç")
                    rag.add_to_knowledge_base(context=chunk, is_from_pdf=True, tags=tags)
                    total_chunks += 1

                    if generate_qa:
                        qa_pairs = self._generate_qa_pairs(chunk)
                        logger.info("–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ %s QA-–ø–∞—Ä –¥–ª—è —á–∞–Ω–∫–∞", len(qa_pairs))
                        for question, answer in qa_pairs:
                            logger.debug("–î–æ–±–∞–≤–ª—è–µ—Ç—Å—è QA-–ø–∞—Ä–∞: –í–æ–ø—Ä–æ—Å: %s, –û—Ç–≤–µ—Ç: %s...",
                                        question[:50], answer[:50])
                            rag.add_to_knowledge_base(question, answer, is_from_pdf=True, tags=tags)
                            total_qa_pairs += 1

                self._save_file_metadata(
                    filename=message.document.file_name,
                    file_type=original_file_type,
                    chunks_count=total_chunks
                )
                logger.info("–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: %s, —á–∞–Ω–∫–æ–≤: %s",
                           message.document.file_name, total_chunks)

                #await self._send_extracted_text_to_admin(bot, message.document.file_name, [chunk for chunk, _ in text_chunks])
                logger.debug("–ò–∑–≤–ª–µ—á—ë–Ω–Ω—ã–µ —á–∞–Ω–∫–∏ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω—ã –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä—É")

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —á–∞–Ω–∫–æ–≤ –≤ –ª–æ–≥-—Ñ–∞–π–ª –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
                log_file = f"extracted_{message.document.file_name.replace('.pdf', '').replace('.txt', '')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
                with open(log_file, 'w', encoding='utf-8') as f:
                    f.write(f"–§–∞–π–ª: {message.document.file_name}\n–û–±—Ä–∞–±–æ—Ç–∞–Ω: {datetime.now().isoformat()}\n\n")
                    for i, (chunk, tags) in enumerate(text_chunks, 1):
                        f.write(f"–ß–∞–Ω–∫ {i} ({len(chunk)} —Å–∏–º–≤–æ–ª–æ–≤, —Ç–µ–≥–∏: {tags or '–Ω–µ—Ç'}):\n{chunk}\n{'-' * 50}\n")
                logger.info("–ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —á–∞–Ω–∫–æ–≤ —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ —Ñ–∞–π–ª: %s", log_file)

                back_keyboard = ReplyKeyboardMarkup(
                    keyboard=[[KeyboardButton(text="–ù–∞–∑–∞–¥")]],
                    resize_keyboard=True
                )
                response = (
                    f"‚úÖ –§–∞–π–ª '{message.document.file_name}' —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω!\n"
                    f"üìö –î–æ–±–∞–≤–ª–µ–Ω–æ {total_chunks} —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤"
                    f"{f' –∏ {total_qa_pairs} –ø–∞—Ä –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç' if generate_qa else ''} –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π.\n"
                    "–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–æ–º–∞–Ω–¥—É /test_file, —á—Ç–æ–±—ã –ø—Ä–æ–≤–µ—Ä–∏—Ç—å, –∫–∞–∫ –±–æ—Ç –æ—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–æ–≥–æ —Ñ–∞–π–ª–∞."
                )
                await message.answer(response, reply_markup=back_keyboard)
                logger.info("–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ: %s", message.document.file_name)
                return "–§–∞–π–ª —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω –∏ –¥–æ–±–∞–≤–ª–µ–Ω –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π"

        except Exception as e:
            logger.error("–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞ %s: %s", message.document.file_name, str(e), exc_info=True)
            return f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞: {e}"

    def _process_large_pdf(self, file_path: str, max_chunk_size: int = 2000, max_pages_per_chunk: int = 10) -> List[
        tuple]:
        text_chunks = []
        current_chunk = ""
        page_count = 0
        total_chars = 0

        keyword_groups = {
            "sleep": ["—Å–æ–Ω", "–±–µ—Å—Å–æ–Ω–Ω–∏—Ü–∞", "—É—Å–Ω—É—Ç—å", "–æ—Ç–¥—ã—Ö", "—Ä–∞—Å—Å–ª–∞–±–ª–µ–Ω–∏–µ"],
            "stress": ["—Å—Ç—Ä–µ—Å—Å", "—Ç—Ä–µ–≤–æ–≥–∞", "–Ω–∞–ø—Ä—è–∂–µ–Ω–∏–µ", "–Ω–µ—Ä–≤—ã", "—É—Å–ø–æ–∫–æ–µ–Ω–∏–µ"],
            "energy": ["—ç–Ω–µ—Ä–≥–∏—è", "–±–æ–¥—Ä–æ—Å—Ç—å", "—É—Å—Ç–∞–ª–æ—Å—Ç—å", "–∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å"],
            "immunity": ["–∏–º–º—É–Ω–∏—Ç–µ—Ç", "–ø—Ä–æ—Å—Ç—É–¥–∞", "–≤–∏—Ä—É—Å—ã", "–∏–º–º—É–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞"],
            "skin": ["–∫–æ–∂–∞", "–∞–∫–Ω–µ", "—É–≤–ª–∞–∂–Ω–µ–Ω–∏–µ", "–ø—Å–æ—Ä–∏–∞–∑"],
            "vitamins": ["–≤–∏—Ç–∞–º–∏–Ω—ã", "–≤–∏—Ç–∞–º–∏–Ω", "–º–∏–Ω–µ—Ä–∞–ª—ã", "–º–∏–∫—Ä–æ—ç–ª–µ–º–µ–Ω—Ç—ã"],
            "weight": ["–≤–µ—Å", "–ø–æ—Ö—É–¥–µ–Ω–∏–µ", "–º–µ—Ç–∞–±–æ–ª–∏–∑–º"],
            "joints": ["—Å—É—Å—Ç–∞–≤—ã", "–∫–æ—Å—Ç–∏", "–∞—Ä—Ç—Ä–∏—Ç", "–≥–∏–±–∫–æ—Å—Ç—å"],
            "gut": ["–∂–∫—Ç", "–ø–∏—â–µ–≤–∞—Ä–µ–Ω–∏–µ", "–∂–µ–ª—É–¥–æ–∫", "–∫–∏—à–µ—á–Ω–∏–∫"]
        }
        logger.debug("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã –≥—Ä—É–ø–ø—ã –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è —Ç–µ–≥–∏—Ä–æ–≤–∞–Ω–∏—è: %s –≥—Ä—É–ø–ø", len(keyword_groups))

        with pdfplumber.open(file_path) as pdf:
            logger.info("–û—Ç–∫—Ä—ã—Ç PDF-—Ñ–∞–π–ª: %s, —Å—Ç—Ä–∞–Ω–∏—Ü: %s", file_path, len(pdf.pages))
            for i, page in enumerate(pdf.pages):
                page_text = page.extract_text() or ""
                logger.debug("–°—Ç—Ä–∞–Ω–∏—Ü–∞ %s: –∏–∑–≤–ª–µ—á–µ–Ω–æ %s —Å–∏–º–≤–æ–ª–æ–≤", i + 1, len(page_text))

                # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–∞–±–ª–∏—Ü
                tables = page.extract_tables()
                if tables:
                    table_text = ""
                    for table in tables:
                        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—É—é —Å—Ç—Ä–æ–∫—É —Ç–∞–±–ª–∏—Ü—ã, –∑–∞–º–µ–Ω—è—è None –Ω–∞ ""
                        table_text += "\n".join(["\t".join(cell if cell is not None else "" for cell in row)
                                                 for row in table if row]) + "\n"
                    page_text += "\n–¢–∞–±–ª–∏—Ü–∞:\n" + table_text
                    logger.info("–°—Ç—Ä–∞–Ω–∏—Ü–∞ %s: –¥–æ–±–∞–≤–ª–µ–Ω —Ç–µ–∫—Å—Ç –∏–∑ %s —Ç–∞–±–ª–∏—Ü, %s —Å–∏–º–≤–æ–ª–æ–≤",
                                i + 1, len(tables), len(table_text))

                if not page_text.strip():
                    logger.warning("–°—Ç—Ä–∞–Ω–∏—Ü–∞ %s: —Ç–µ–∫—Å—Ç –Ω–µ –∏–∑–≤–ª–µ—á—ë–Ω", i + 1)
                    page_text = ""

                total_chars += len(page_text)
                page_count += 1

                if len(current_chunk) + len(page_text) <= max_chunk_size and page_count <= max_pages_per_chunk:
                    current_chunk += page_text + "\n"
                    logger.debug("–°—Ç—Ä–∞–Ω–∏—Ü–∞ %s –¥–æ–±–∞–≤–ª–µ–Ω–∞ –≤ —Ç–µ–∫—É—â–∏–π —á–∞–Ω–∫, —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞: %s",
                                 i + 1, len(current_chunk))
                else:
                    if current_chunk.strip():
                        tags = self._generate_tags(current_chunk, keyword_groups)
                        text_chunks.append((current_chunk.strip(), tags))
                        logger.info("–°–æ–∑–¥–∞–Ω —á–∞–Ω–∫ %s: %s —Å–∏–º–≤–æ–ª–æ–≤, —Ç–µ–≥–∏: %s",
                                    len(text_chunks), len(current_chunk.strip()), tags or "–Ω–µ—Ç")
                    current_chunk = page_text + "\n"
                    page_count = 1
                    logger.debug("–ù–∞—á–∞—Ç –Ω–æ–≤—ã–π —á–∞–Ω–∫ —Å —Å—Ç—Ä–∞–Ω–∏—Ü—ã %s, —Ä–∞–∑–º–µ—Ä: %s",
                                 i + 1, len(current_chunk))

                if i == len(pdf.pages) - 1 and current_chunk.strip():
                    tags = self._generate_tags(current_chunk, keyword_groups)
                    text_chunks.append((current_chunk.strip(), tags))
                    logger.info("–°–æ–∑–¥–∞–Ω —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —á–∞–Ω–∫ %s: %s —Å–∏–º–≤–æ–ª–æ–≤, —Ç–µ–≥–∏: %s",
                                len(text_chunks), len(current_chunk.strip()), tags or "–Ω–µ—Ç")

        logger.info("PDF –æ–±—Ä–∞–±–æ—Ç–∞–Ω: %s —á–∞–Ω–∫–æ–≤, –≤—Å–µ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–æ %s —Å–∏–º–≤–æ–ª–æ–≤", len(text_chunks), total_chars)
        return text_chunks

    def _generate_tags(self, text: str, keyword_groups: dict) -> str:
        text_lower = text.lower()
        tags = []
        for group, keywords in keyword_groups.items():
            if any(kw in text_lower for kw in keywords):
                tags.append(group)
        result = ",".join(tags) if tags else None
        logger.debug("–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã —Ç–µ–≥–∏ –¥–ª—è —Ç–µ–∫—Å—Ç–∞ (%s —Å–∏–º–≤–æ–ª–æ–≤): %s", len(text), result or "–Ω–µ—Ç")
        return result

    def _generate_qa_pairs(self, text: str) -> List[tuple]:
        qa_pairs = []
        chunks = self._split_text_into_chunks(text, chunk_size=1500)
        logger.info("–ß–∞–Ω–∫ —Ä–∞–∑–±–∏—Ç –Ω–∞ %s –ø–æ–¥—á–∞–Ω–∫–æ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ QA-–ø–∞—Ä", len(chunks))

        for i, chunk in enumerate(chunks):
            logger.debug("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è QA-–ø–∞—Ä –¥–ª—è –ø–æ–¥—á–∞–Ω–∫–∞ %s: %s —Å–∏–º–≤–æ–ª–æ–≤", i + 1, len(chunk))
            prompt = (
                f"–ù–∞ –æ—Å–Ω–æ–≤–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–µ–∫—Å—Ç–∞ —Å–æ–∑–¥–∞–π 5-10 –ø–∞—Ä –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –ø–∏—Ç–∞–Ω–∏–µ–º –∏–ª–∏ –∑–¥–æ—Ä–æ–≤—å–µ–º:\n\n"
                f"{chunk}\n\n"
                "–í–æ–ø—Ä–æ—Å—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–º–∏, –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º–∏ –∏ –ø–æ–ª–µ–∑–Ω—ã–º–∏, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–º–∏ —Ç–æ–ª—å–∫–æ –Ω–∞ —Ç–µ–∫—Å—Ç–µ. "
                "–ü—Ä–∏–º–µ—Ä—ã: '–ö–∞–∫–æ–π –ë–ê–î –ø–æ–º–æ–≥–∞–µ—Ç –ø—Ä–∏ –≤—ã–ø–∞–¥–µ–Ω–∏–∏ –≤–æ–ª–æ—Å?', '–ß—Ç–æ –ø–æ–ª–µ–∑–Ω–æ –¥–ª—è –∏–º–º—É–Ω–∏—Ç–µ—Ç–∞?'. "
                "–û—Ç–≤–µ—Ç—ã ‚Äî –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–º–∏, —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—ã–º–∏, —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —ç–º–æ–¥–∑–∏ (üåü, üìù, ‚úÖ) –∏ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤. "
                "–ï—Å–ª–∏ —Ç–µ–∫—Å—Ç–∞ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ, –≤–µ—Ä–Ω–∏ –ø—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç.\n"
                "–§–æ—Ä–º–∞—Ç:\nQ: [–≤–æ–ø—Ä–æ—Å]\nA: [–æ—Ç–≤–µ—Ç]\n"
            )
            response = self._call_llm(prompt)
            if response:
                pairs = self._parse_qa_response(response)
                filtered_pairs = [(q, a) for q, a in pairs if q and a and len(q) > 10 and len(a) > 50]
                qa_pairs.extend(filtered_pairs[:7])
                logger.info("–ü–æ–¥—á–∞–Ω–∫ %s: —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ %s QA-–ø–∞—Ä", i + 1, len(filtered_pairs))
            else:
                logger.warning("–ü–æ–¥—á–∞–Ω–∫ %s: –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å QA-–ø–∞—Ä—ã", i + 1)

        logger.info("–í—Å–µ–≥–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ %s QA-–ø–∞—Ä –∏–∑ —Ç–µ–∫—Å—Ç–∞", len(qa_pairs))
        return qa_pairs

    def _call_llm(self, prompt: str) -> Optional[str]:
        logger.debug("–û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ LLM, –¥–ª–∏–Ω–∞ –ø—Ä–æ–º–ø—Ç–∞: %s —Å–∏–º–≤–æ–ª–æ–≤", len(prompt))
        try:
            headers = {"Content-Type": "application/json"}
            data = {
                "model": "llama3.1:8b",
                "prompt": prompt,
                "system": (
                    "–¢—ã - –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –Ω—É—Ç—Ä–∏—Ü–∏–æ–ª–æ–≥. –û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ. "
                    "–ë—É–¥—å –≤–µ–∂–ª–∏–≤—ã–º, –∏—Å–ø–æ–ª—å–∑—É–π —ç–º–æ–¥–∑–∏ –∏ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è –∫—Ä–∞—Å–∏–≤–æ–≥–æ –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏—è."
                ),
                "stream": False,
                "options": {"temperature": 0.7, "max_tokens": 1500}
            }
            response = requests.post(self.llm_url, headers=headers, json=data)
            response.raise_for_status()
            result = response.json().get("response")
            logger.info("LLM –≤–µ—Ä–Ω—É–ª –æ—Ç–≤–µ—Ç: %s —Å–∏–º–≤–æ–ª–æ–≤", len(result) if result else 0)
            return result
        except Exception as e:
            logger.error("–û—à–∏–±–∫–∞ –≤—ã–∑–æ–≤–∞ LLM: %s", str(e))
            return None

    def _parse_qa_response(self, response: str) -> List[tuple]:
        qa_pairs = []
        lines = response.strip().split('\n')
        question = None
        for line in lines:
            if line.startswith("Q:"):
                question = line[2:].strip()
                logger.debug("–ò–∑–≤–ª–µ—á—ë–Ω –≤–æ–ø—Ä–æ—Å: %s...", question[:50])
            elif line.startswith("A:") and question:
                answer = line[2:].strip()
                qa_pairs.append((question, answer))
                logger.debug("–ò–∑–≤–ª–µ—á—ë–Ω –æ—Ç–≤–µ—Ç: %s...", answer[:50])
                question = None
        logger.info("–†–∞—Å–ø–∞—Ä—Å–µ–Ω–æ %s QA-–ø–∞—Ä –∏–∑ –æ—Ç–≤–µ—Ç–∞ LLM", len(qa_pairs))
        return qa_pairs

    def _split_text_into_chunks(self, text: str, chunk_size: int = 2000) -> List[str]:
        sentences = sent_tokenize(text, language="russian")
        chunks = []
        current_chunk = ""
        for sentence in sentences:
            if len(current_chunk) + len(sentence) <= chunk_size:
                current_chunk += sentence + " "
            else:
                if current_chunk.strip():
                    chunks.append(current_chunk.strip())
                    logger.debug("–°–æ–∑–¥–∞–Ω —á–∞–Ω–∫: %s —Å–∏–º–≤–æ–ª–æ–≤", len(current_chunk.strip()))
                current_chunk = sentence + " "
        if current_chunk.strip():
            chunks.append(current_chunk.strip())
            logger.debug("–°–æ–∑–¥–∞–Ω —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —á–∞–Ω–∫: %s —Å–∏–º–≤–æ–ª–æ–≤", len(current_chunk.strip()))
        logger.info("–¢–µ–∫—Å—Ç —Ä–∞–∑–±–∏—Ç –Ω–∞ %s —á–∞–Ω–∫–æ–≤", len(chunks))
        return chunks

    def _save_file_metadata(self, filename: str, file_type: str, chunks_count: int):
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute('''INSERT INTO training_files 
                        (filename, file_type, processed_at, chunks_count)
                        VALUES (?, ?, ?, ?)''',
                       (filename, file_type, datetime.now().isoformat(), chunks_count))
        conn.commit()
        conn.close()
        logger.info("–°–æ—Ö—Ä–∞–Ω–µ–Ω—ã –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ: —Ñ–∞–π–ª=%s, —Ç–∏–ø=%s, —á–∞–Ω–∫–æ–≤=%s", filename, file_type, chunks_count)

    async def _send_extracted_text_to_admin(self, bot: Bot, filename: str, text_chunks: List[str]):
        admin_id = 5440647148
        header = f"üìÑ –û–±—Ä–∞–±–æ—Ç–∞–Ω —Ñ–∞–π–ª: {filename}\n\n‚úÇÔ∏è –ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã:\n{'=' * 20}\n"
        total_chars = sum(len(chunk) for chunk in text_chunks)
        logger.info("–û—Ç–ø—Ä–∞–≤–∫–∞ %s —á–∞–Ω–∫–æ–≤ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä—É, –≤—Å–µ–≥–æ %s —Å–∏–º–≤–æ–ª–æ–≤", len(text_chunks), total_chars)

        if len(text_chunks) > 10:
            summary = f"–í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤: {len(text_chunks)}\n–ü–µ—Ä–≤—ã–µ 10:\n"
            chunks_to_send = text_chunks[:10]
        else:
            summary = ""
            chunks_to_send = text_chunks

        full_text = "\n\n".join([f"–ß–∞–Ω–∫ {i+1} ({len(chunk)} —Å–∏–º–≤–æ–ª–æ–≤):\n{chunk[:500]}..."
                                for i, chunk in enumerate(chunks_to_send)])
        message_text = header + summary + full_text

        if len(message_text) <= 4096:
            await bot.send_message(chat_id=admin_id, text=message_text)
            logger.debug("–û—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ –æ–¥–Ω–æ —Å–æ–æ–±—â–µ–Ω–∏–µ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä—É: %s —Å–∏–º–≤–æ–ª–æ–≤", len(message_text))
        else:
            await bot.send_message(chat_id=admin_id, text=header + summary)
            logger.debug("–û—Ç–ø—Ä–∞–≤–ª–µ–Ω –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä—É")
            for i, chunk in enumerate(chunks_to_send):
                chunk_preview = f"–ß–∞–Ω–∫ {i+1} ({len(chunk)} —Å–∏–º–≤–æ–ª–æ–≤):\n{chunk[:500]}..."
                for j in range(0, len(chunk_preview), 4096):
                    part = chunk_preview[j:j + 4096]
                    await bot.send_message(chat_id=admin_id, text=part)
                    logger.debug("–û—Ç–ø—Ä–∞–≤–ª–µ–Ω–∞ —á–∞—Å—Ç—å —á–∞–Ω–∫–∞ %s: %s —Å–∏–º–≤–æ–ª–æ–≤", i + 1, len(part))

    def get_training_stats(self) -> Optional[str]:
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            cursor.execute('''SELECT COUNT(*) FROM training_files''')
            total_files = cursor.fetchone()[0]
            cursor.execute('''SELECT SUM(chunks_count) FROM training_files''')
            total_chunks = cursor.fetchone()[0] or 0
            cursor.execute('''SELECT filename, processed_at 
                            FROM training_files 
                            ORDER BY processed_at DESC 
                            LIMIT 5''')
            recent_files = cursor.fetchall()
            conn.close()

            stats = [
                "üìä **–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—É—á–µ–Ω–∏—è –±–æ—Ç–∞**",
                f"üìö –í—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {total_files}",
                f"‚úÇÔ∏è –í—Å–µ–≥–æ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤: {total_chunks}",
                "\nüîç **–ü–æ—Å–ª–µ–¥–Ω–∏–µ 5 —Ñ–∞–π–ª–æ–≤:**"
            ]
            for i, (filename, date) in enumerate(recent_files, 1):
                stats.append(f"{i}. {filename} (–¥–∞—Ç–∞: {date[:10]})")
            logger.info("–°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—É—á–µ–Ω–∏—è: %s —Ñ–∞–π–ª–æ–≤, %s —á–∞–Ω–∫–æ–≤", total_files, total_chunks)
            return "\n".join(stats)
        except Exception as e:
            logger.error("–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è: %s", str(e))
            return None